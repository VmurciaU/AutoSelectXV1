{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda32f53",
   "metadata": {},
   "source": [
    "Paso 0 ‚Äî Reset seguro de la colecci√≥n (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a249a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 0: Reset opcional de la colecci√≥n ---\n",
    "\n",
    "import os, chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "VDB_PATH = \"./vectordb\"\n",
    "COLLECTION_NAME = \"autoselx_docs\"\n",
    "\n",
    "def reset_collection(full_reset=False, collection_name=COLLECTION_NAME):\n",
    "    import chromadb\n",
    "    if full_reset:\n",
    "        if os.path.exists(VDB_PATH):\n",
    "            shutil.rmtree(VDB_PATH)\n",
    "            print(f\"üßπ Directorio {VDB_PATH} borrado por completo.\")\n",
    "        os.makedirs(VDB_PATH, exist_ok=True)\n",
    "    else:\n",
    "        client = chromadb.PersistentClient(path=VDB_PATH)\n",
    "        try:\n",
    "            client.delete_collection(name=collection_name)\n",
    "            print(f\"üßπ Colecci√≥n '{collection_name}' eliminada.\")\n",
    "        except Exception as e:\n",
    "            print(f\"(info) No se elimin√≥ colecci√≥n: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 1: imports y utilidades ---\n",
    "import os, re, pdfplumber\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document   # ‚úÖ Import correcto\n",
    "\n",
    "# Verificaci√≥n de API KEY (ajusta si usas dotenv)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables del archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Verificaci√≥n de API KEY\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ùå Falta configurar la variable OPENAI_API_KEY en tu entorno o archivo .env\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# --- Patrones de limpieza (encabezados/pies repetidos) ---\n",
    "CLEAN_PATTERNS = [\n",
    "    r\"FORMATO REQUISICI√ìN.*\",\n",
    "    r\"ECOPETROL DESARROLLO DE PROYECTO.*\",\n",
    "    r\"Todos los derechos reservados.*\",\n",
    "    r\"EDP-F-046.*Versi√≥n.*\",\n",
    "    r\"MR.*#:.*CAS.*\",\n",
    "    r\"P√°gina\\s+\\d+\\s+de\\s+\\d+\",\n",
    "    r\"_{6,}\",  # l√≠neas largas\n",
    "]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    for p in CLEAN_PATTERNS:\n",
    "        text = re.sub(p, \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# --- Palabras clave para asignar prioridad ---\n",
    "HD_HINTS = [\n",
    "    r\"\\bAPI\\s*675\\b\", r\"HOJA\\s*DE\\s*DATOS\", r\"\\bDATA\\s*SHEET\\b\", r\"\\bDATASHEET\\b\",\n",
    "    r\"BOMB(A|S)?\\s+(DOSIFICADORA|DOSING)\"\n",
    "]\n",
    "\n",
    "ALCANCE_HINTS = [\n",
    "    r\"\\bALCANCE\\b\", r\"ALCANCE\\s+DEL\\s+SUMINISTRO\", r\"\\bSCOPE\\b\", r\"SCOPE\\s+OF\\s+SUPPLY\"\n",
    "]\n",
    "\n",
    "REQ_HINTS = [\n",
    "    r\"\\bREQUISITO(S)?\\b\", r\"\\bREQUERIMIENTO(S)?\\b\",\n",
    "    r\"REQUISITO(S)?\\s+T(√â|E)CNICO(S)?\", r\"CONDICIONES?\\s+T(√â|E)CNICAS?\",\n",
    "    r\"CRITERIOS?\\s+DE\\s+DISE(√ë|N)O\", r\"DATOS?\\s+DE\\s+DISE(√ë|N)O\",\n",
    "    r\"\\bREQUIREMENT(S)?\\b\", r\"DESIGN\\s+REQUIREMENT(S)?\\b\",\n",
    "    r\"TECHNICAL\\s+REQUIREMENT(S)?\\b\", r\"\\bSPECIFICATION(S)?\\b\",\n",
    "    r\"MINIMUM\\s+REQUIREMENT(S)?\\b\",\n",
    "    r\"\\bEL\\s+PROVEEDOR\\s+DEBE\\b\", r\"\\bEL\\s+VENDEDOR\\s+DEBE\\b\",\n",
    "    r\"\\bDEBER√Å\\b\", r\"\\bSE\\s+REQUIERE(N)?\\b\",\n",
    "    r\"\\bSHALL\\b\", r\"\\bMUST\\b\", r\"\\bIS\\s+REQUIRED\\s+TO\\b\"\n",
    "]\n",
    "\n",
    "NORM_HINTS = [\n",
    "    r\"\\bNORMAS?\\b\", r\"\\bEST[√ÅA]NDA(RE|R)E?S?\\b\",\n",
    "    r\"\\bC(√ì|O)DIGO(S)?\\b\", r\"\\bCODE(S)?\\b\", r\"\\bSTANDARD(S)?\\b\",\n",
    "    r\"\\bSPEC(S|IFICATIONS)?\\b\", r\"\\bREGULATION(S)?\\b\",\n",
    "    r\"\\bRETIE\\b\", r\"\\bAPI\\b\", r\"\\bASTM\\b\", r\"\\bASME\\b\", r\"\\bIEC\\b\", r\"\\bIEEE\\b\",\n",
    "    r\"APPLICABLE\\s+(CODES|STANDARDS|SPECIFICATIONS)\"\n",
    "]\n",
    "\n",
    "def matches_any(text: str, patterns: List[str]) -> bool:\n",
    "    return any(re.search(p, text, flags=re.IGNORECASE) for p in patterns)\n",
    "\n",
    "def detect_priority(text: str) -> str:\n",
    "    \"\"\"Clasifica el chunk en una categor√≠a fija de prioridad\"\"\"\n",
    "    if matches_any(text, HD_HINTS):      return \"hoja_datos\"\n",
    "    if matches_any(text, ALCANCE_HINTS): return \"alcance\"\n",
    "    if matches_any(text, REQ_HINTS):     return \"requisito\"\n",
    "    if matches_any(text, NORM_HINTS):    return \"norma\"\n",
    "    return \"general\"\n",
    "\n",
    "# --- Conversi√≥n de tablas ---\n",
    "def table_to_text_and_kv(rows: List[List[str]]) -> Tuple[str, Dict[str, str] | None]:\n",
    "    clean_rows = []\n",
    "    for r in rows:\n",
    "        if not r:\n",
    "            continue\n",
    "        rr = [(c or \"\").strip() for c in r]\n",
    "        if any(rr):\n",
    "            clean_rows.append(rr)\n",
    "\n",
    "    kv = None\n",
    "    if clean_rows and max(len(r) for r in clean_rows) <= 2 and len(clean_rows) >= 2:\n",
    "        kv = {}\n",
    "        for r in clean_rows:\n",
    "            if len(r) >= 2 and r[0] and r[1]:\n",
    "                kv[r[0]] = r[1]\n",
    "\n",
    "    text = \"\\n\".join(\" | \".join(r) for r in clean_rows)\n",
    "    return text, kv if kv else None\n",
    "\n",
    "# --- KV extractor unificado (texto + tablas) ---\n",
    "def kv_from_text_page(text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extrae pares clave-valor t√≠picos de Hoja de Datos (texto o tabla).\n",
    "    Soporta caudal, presi√≥n, potencia, viscosidad, redundancia, materiales, API 675.\n",
    "    \"\"\"\n",
    "    t = \" \".join(text.split()).upper()\n",
    "    kv = {}\n",
    "\n",
    "    # Caudal (GPH o LPH, con rangos)\n",
    "    m = re.search(r\"(\\d+(?:[.,]\\d+)?)\\s*(?:-|‚Äì|A|TO)?\\s*(\\d+(?:[.,]\\d+)?)?\\s*GPH\", t)\n",
    "    if m: kv[\"caudal_gph\"] = f\"{m.group(1)}-{m.group(2)}\" if m.group(2) else m.group(1)\n",
    "\n",
    "    m = re.search(r\"(\\d+(?:[.,]\\d+)?)\\s*(?:-|‚Äì|A|TO)?\\s*(\\d+(?:[.,]\\d+)?)?\\s*LPH\", t)\n",
    "    if m: kv[\"caudal_lph\"] = f\"{m.group(1)}-{m.group(2)}\" if m.group(2) else m.group(1)\n",
    "\n",
    "    # Presi√≥n (PSI o BAR, con rangos)\n",
    "    m = re.search(r\"(\\d+(?:[.,]\\d+)?)\\s*(?:-|‚Äì|A|TO)?\\s*(\\d+(?:[.,]\\d+)?)?\\s*PSI\", t)\n",
    "    if m: kv[\"presion_psig\"] = f\"{m.group(1)}-{m.group(2)}\" if m.group(2) else m.group(1)\n",
    "\n",
    "    m = re.search(r\"(\\d+(?:[.,]\\d+)?)\\s*(?:-|‚Äì|A|TO)?\\s*(\\d+(?:[.,]\\d+)?)?\\s*BAR\", t)\n",
    "    if m: kv[\"presion_bar\"] = f\"{m.group(1)}-{m.group(2)}\" if m.group(2) else m.group(1)\n",
    "\n",
    "    # Potencia (HP o KW)\n",
    "    m = re.search(r\"(\\d+(?:[.,]\\d+)?)\\s*HP\", t)\n",
    "    if m: kv[\"hp_motor\"] = m.group(1)\n",
    "\n",
    "    m = re.search(r\"(\\d+(?:[.,]\\d+)?)\\s*KW\", t)\n",
    "    if m: kv[\"kw_motor\"] = m.group(1)\n",
    "\n",
    "    # Viscosidad (cps)\n",
    "    m = re.search(r\"(\\d+)\\s*-\\s*(\\d+)\\s*CPS\", t)\n",
    "    if m:\n",
    "        kv[\"rango_cps_min\"] = m.group(1)\n",
    "        kv[\"rango_cps_max\"] = m.group(2)\n",
    "\n",
    "    # Bombas operativas + respaldo (ej. 1+1)\n",
    "    m = re.search(r\"\\b(\\d+)\\s*\\+\\s*(\\d+)\\b\", t)\n",
    "    if m:\n",
    "        kv[\"bombas_operativas\"] = m.group(1)\n",
    "        kv[\"bombas_respaldo\"] = m.group(2)\n",
    "\n",
    "    # Materiales\n",
    "    if \"304\" in t: kv[\"material_tanque\"] = \"304SS\"\n",
    "    if \"316\" in t: kv[\"material_mojado\"] = \"316SS\"\n",
    "\n",
    "    # Norma API 675\n",
    "    if \"API 675\" in t: kv[\"api_675\"] = \"true\"\n",
    "\n",
    "    return kv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48671a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# --- Paso 2: extracci√≥n de contenido del PDF (mejorado) ---\n",
    "import json\n",
    "import pdfplumber\n",
    "from typing import List\n",
    "from langchain_core.documents import Document  # ‚úÖ Import correcto\n",
    "\n",
    "def _is_text_clean(text: str, min_ratio: float = 0.7) -> bool:\n",
    "    \"\"\"Chequea que el texto tenga mayor√≠a de caracteres legibles (no ruido PDF).\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    clean_chars = sum(c.isalnum() or c.isspace() or c in \".,;:()-/%\" for c in text)\n",
    "    ratio = clean_chars / len(text)\n",
    "    return ratio >= min_ratio\n",
    "\n",
    "def extract_pdf_content(filepath: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extrae texto y tablas del PDF con metadatos enriquecidos:\n",
    "    - type: text/table\n",
    "    - prio: hoja_datos, alcance, requisito, norma, general\n",
    "    - kv: si se detecta informaci√≥n t√©cnica\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "\n",
    "            # --- 1) Texto plano ---\n",
    "            raw_text = page.extract_text() or \"\"\n",
    "            clean = clean_text(raw_text)\n",
    "\n",
    "            if clean and _is_text_clean(clean):\n",
    "                prio = detect_priority(clean)\n",
    "\n",
    "                # ‚öñÔ∏è Ajuste: si contiene \"ALCANCE\", fuerza prioridad a alcance\n",
    "                if \"ALCANCE\" in clean.upper():\n",
    "                    prio = \"alcance\"\n",
    "\n",
    "                md = {\"source\": filepath, \"page\": i, \"type\": \"text\", \"prio\": prio}\n",
    "\n",
    "                # üîë Extraer KV desde texto\n",
    "                kv_text = kv_from_text_page(clean)\n",
    "                if kv_text:\n",
    "                    md[\"kv\"] = json.dumps(kv_text, ensure_ascii=False)\n",
    "\n",
    "                docs.append(Document(page_content=clean, metadata=md))\n",
    "\n",
    "            # --- 2) Tablas ---\n",
    "            try:\n",
    "                tables = page.extract_tables() or []\n",
    "            except Exception:\n",
    "                tables = []\n",
    "\n",
    "            for trows in tables:\n",
    "                # Convertimos tabla en texto legible y posible dict KV directo\n",
    "                table_text, kv_table = table_to_text_and_kv(trows)\n",
    "                table_text = clean_text(table_text)\n",
    "\n",
    "                if not table_text or not _is_text_clean(table_text):\n",
    "                    continue\n",
    "\n",
    "                prio = detect_priority(table_text)\n",
    "\n",
    "                # ‚öñÔ∏è Ajuste igual que arriba\n",
    "                if \"ALCANCE\" in table_text.upper():\n",
    "                    prio = \"alcance\"\n",
    "\n",
    "                md = {\"source\": filepath, \"page\": i, \"type\": \"table\", \"prio\": prio}\n",
    "\n",
    "                # üîë Detectar KV desde texto y tabla\n",
    "                kv_detected = kv_from_text_page(table_text)\n",
    "                if kv_table:\n",
    "                    kv_detected.update(kv_table)\n",
    "\n",
    "                if kv_detected:\n",
    "                    md[\"kv\"] = json.dumps(kv_detected, ensure_ascii=False)\n",
    "\n",
    "                docs.append(Document(page_content=table_text, metadata=md))\n",
    "\n",
    "    return docs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 2: extracci√≥n de contenido del PDF (robusto a p√°ginas con tablas) --- PRUEBAAAAAAAAAa\n",
    "import json, re, pdfplumber\n",
    "from typing import List\n",
    "from langchain_core.documents import Document  # ‚úÖ\n",
    "\n",
    "# --- Heur√≠stica: detectar texto \"corrupto/gibberish\" t√≠pico de tablas mal le√≠das ---\n",
    "def is_gibberish_text(t: str) -> bool:\n",
    "    if not t or not t.strip():\n",
    "        return True\n",
    "    s = \" \".join(t.split())\n",
    "\n",
    "    # 1) Muchos puntos seguidos (l√≠neas de puntos / leaders)\n",
    "    if re.search(r\"\\.{5,}\", s):\n",
    "        return True\n",
    "\n",
    "    # 2) Secuencias largas de letras sueltas separadas por espacios\n",
    "    if re.search(r\"(?:\\b\\w\\b\\s+){6,}\", s):\n",
    "        return True\n",
    "\n",
    "    # 3) Ratio de tokens de una sola letra elevado\n",
    "    tokens = re.findall(r\"[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]+\", s)\n",
    "    if tokens:\n",
    "        single = sum(1 for tok in tokens if len(tok) == 1)\n",
    "        if single / max(len(tokens), 1) > 0.35:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def extract_pdf_content(filepath: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extrae texto y tablas con metadatos:\n",
    "      - type: text | table\n",
    "      - prio: hoja_datos | alcance | requisito | norma | general\n",
    "      - kv: dict (serializado) cuando se detecta info t√©cnica\n",
    "    Reglas:\n",
    "      ‚Ä¢ Si la p√°gina tiene tablas y el texto plano luce \"gibberish\", no indexar el texto.\n",
    "      ‚Ä¢ Para tablas: convertimos a texto legible y combinamos KV por estructura y por regex.\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            # --- 1) Tablas primero (sabemos si hay tablas para decidir sobre texto) ---\n",
    "            try:\n",
    "                tables = page.extract_tables() or []\n",
    "            except Exception:\n",
    "                tables = []\n",
    "            has_tables = bool(tables)\n",
    "\n",
    "            table_docs_this_page: List[Document] = []\n",
    "            for trows in tables:\n",
    "                table_text, kv_table = table_to_text_and_kv(trows)\n",
    "                table_text = clean_text(table_text)\n",
    "                if not table_text:\n",
    "                    continue\n",
    "\n",
    "                prio = detect_priority(table_text)\n",
    "                md = {\"source\": filepath, \"page\": i, \"type\": \"table\", \"prio\": prio}\n",
    "\n",
    "                # KV por regex + por estructura de 2 columnas\n",
    "                kv_detected = kv_from_text_page(table_text)\n",
    "                if kv_table:\n",
    "                    kv_detected = {**kv_detected, **kv_table}\n",
    "                if kv_detected:\n",
    "                    md[\"kv\"] = json.dumps(kv_detected, ensure_ascii=False)\n",
    "\n",
    "                table_docs_this_page.append(Document(page_content=table_text, metadata=md))\n",
    "\n",
    "            # --- 2) Texto plano (solo si vale la pena) ---\n",
    "            raw_text = page.extract_text() or \"\"\n",
    "            clean = clean_text(raw_text)\n",
    "            text_looks_bad = is_gibberish_text(clean)\n",
    "\n",
    "            # Si hay tablas y el texto luce corrupto, NO a√±adimos el texto de la p√°gina\n",
    "            add_text = bool(clean) and not (has_tables and text_looks_bad)\n",
    "\n",
    "            if add_text:\n",
    "                prio = detect_priority(clean)\n",
    "                md = {\"source\": filepath, \"page\": i, \"type\": \"text\", \"prio\": prio}\n",
    "\n",
    "                kv_text = kv_from_text_page(clean)\n",
    "                if kv_text:\n",
    "                    md[\"kv\"] = json.dumps(kv_text, ensure_ascii=False)\n",
    "\n",
    "                docs.append(Document(page_content=clean, metadata=md))\n",
    "\n",
    "            # --- 3) A√±adir las tablas (siempre que tengan contenido) ---\n",
    "            docs.extend(table_docs_this_page)\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf126d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 3 - Indexar en Chroma (persistente)\n",
    "# ============================================\n",
    "from collections import Counter\n",
    "from langchain_chroma import Chroma   # ‚úÖ usar langchain_chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Configuraci√≥n global\n",
    "VDB_PATH = \"./vectordb\"\n",
    "COLLECTION_NAME = \"autoselx_docs\"\n",
    "\n",
    "def reset_collection():\n",
    "    \"\"\"Elimina la colecci√≥n persistente (para recargar desde cero).\"\"\"\n",
    "    import shutil, os\n",
    "    if os.path.exists(VDB_PATH):\n",
    "        shutil.rmtree(VDB_PATH)\n",
    "        print(f\"üóëÔ∏è Colecci√≥n eliminada en {VDB_PATH}\")\n",
    "\n",
    "def _safe_persist(vs):\n",
    "    \"\"\"Compatibilidad de persistencia entre versiones de Chroma/langchain.\"\"\"\n",
    "    try:\n",
    "        if hasattr(vs, \"persist\"):\n",
    "            vs.persist()  # versiones antiguas\n",
    "        elif hasattr(vs, \"_client\") and hasattr(vs._client, \"persist\"):\n",
    "            vs._client.persist()  # versiones nuevas\n",
    "    except Exception as e:\n",
    "        print(f\"(i) No se pudo forzar persistencia expl√≠cita: {e}\")\n",
    "\n",
    "def add_files_to_vectordb(filepath: str, reset: bool = False):\n",
    "    \"\"\"\n",
    "    1) Extrae documentos del PDF (texto + tablas + KV)\n",
    "    2) Aplica chunking conservando metadatos\n",
    "    3) Indexa en Chroma persistente\n",
    "    4) Muestra diagn√≥stico resumido\n",
    "    \"\"\"\n",
    "    if reset:\n",
    "        reset_collection()\n",
    "\n",
    "    # --- 1) Extraer documentos brutos ---\n",
    "    raw_docs = extract_pdf_content(filepath)\n",
    "\n",
    "    # --- 2) Chunking ---\n",
    "    splits = chunk_documents(raw_docs)\n",
    "\n",
    "    # --- 3) Indexar en Chroma persistente ---\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=VDB_PATH,\n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "\n",
    "    # üîë Persistir en disco (seguro en m√∫ltiples versiones)\n",
    "    _safe_persist(vectorstore)\n",
    "\n",
    "    # --- 4) Diagn√≥stico ---\n",
    "    by_type = Counter(d.metadata.get(\"type\", \"text\") for d in splits)\n",
    "    by_prio = Counter(d.metadata.get(\"prio\", \"general\") for d in splits)\n",
    "    total_kv = sum(1 for d in splits if \"kv\" in d.metadata)\n",
    "\n",
    "    try:\n",
    "        n_in_db = vectorstore._collection.count()\n",
    "        print(f\"üì¶ Documentos en DB (colecci√≥n real): {n_in_db}\")\n",
    "    except Exception:\n",
    "        print(\"(i) No se pudo obtener conteo directo desde DB\")\n",
    "\n",
    "    print(f\"‚úÖ {len(splits)} fragmentos indexados en {VDB_PATH} (colecci√≥n '{COLLECTION_NAME}')\")\n",
    "    print(\"   üìä Por tipo:\", dict(by_type))\n",
    "    print(\"   üìä Por prioridad:\", dict(by_prio))\n",
    "    print(f\"   üîë Documentos con KV extra√≠dos: {total_kv}\")\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682767b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 4 - Carga incremental y diagn√≥stico extendido\n",
    "# ============================================\n",
    "from collections import Counter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# === Utilidades de normalizaci√≥n y hashing  ===\n",
    "import hashlib\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def _normalize_for_hash(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def make_doc_id(meta: dict, content: str) -> str:\n",
    "    \"\"\"ID estable basado en (source|page|type|contenido normalizado).\"\"\"\n",
    "    key = f\"{meta.get('source','')}|{meta.get('page','')}|{meta.get('type','')}|{_normalize_for_hash(content)}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def dedupe_by_hash(docs: List[Document]) -> tuple[list[Document], list[str], int]:\n",
    "    \"\"\"Elimina duplicados por contenido normalizado. Devuelve (docs_unicos, ids, n_drops).\"\"\"\n",
    "    seen = set()\n",
    "    out_docs, ids = [], []\n",
    "    drops = 0\n",
    "    for d in docs:\n",
    "        h = make_doc_id(d.metadata, d.page_content)\n",
    "        if h in seen:\n",
    "            drops += 1\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        # Guarda el hash en metadatos (√∫til para debug)\n",
    "        d.metadata[\"content_hash\"] = h\n",
    "        out_docs.append(d)\n",
    "        ids.append(h)\n",
    "    return out_docs, ids, drops\n",
    "\n",
    "\n",
    "\n",
    "def add_files_to_vectordb(filepath: str, reset: bool = False):\n",
    "    if reset:\n",
    "        reset_collection()\n",
    "\n",
    "    # 1) Extraer\n",
    "    raw_docs = extract_pdf_content(filepath)\n",
    "\n",
    "    # 2) Chunking\n",
    "    splits = chunk_documents(raw_docs)\n",
    "\n",
    "    # 2.1) Dedupe por contenido\n",
    "    unique_docs, ids, dropped = dedupe_by_hash(splits)\n",
    "\n",
    "    # 3) Indexar con IDs deterministas\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=unique_docs,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=VDB_PATH,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        ids=ids,  # üëà clave\n",
    "    )\n",
    "\n",
    "    _safe_persist(vectorstore)\n",
    "\n",
    "    # 4) Diagn√≥stico\n",
    "    by_type = Counter(d.metadata.get(\"type\", \"text\") for d in unique_docs)\n",
    "    by_prio = Counter(d.metadata.get(\"prio\", \"general\") for d in unique_docs)\n",
    "    total_kv = sum(1 for d in unique_docs if \"kv\" in d.metadata)\n",
    "\n",
    "    try:\n",
    "        n_in_db = vectorstore._collection.count()\n",
    "        print(f\"üì¶ Documentos en DB (colecci√≥n real): {n_in_db}\")\n",
    "    except Exception:\n",
    "        n_in_db = None\n",
    "\n",
    "    print(f\"‚úÖ {len(unique_docs)} fragmentos (tras dedupe; {dropped} descartados) indexados en {VDB_PATH} (colecci√≥n '{COLLECTION_NAME}')\")\n",
    "    print(\"   üìä Por tipo:\", dict(by_type))\n",
    "    print(\"   üìä Por prioridad:\", dict(by_prio))\n",
    "    print(f\"   üîë Documentos con KV extra√≠dos: {total_kv}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 5 - Utilidad para inspeccionar colecci√≥n\n",
    "# ============================================\n",
    "from collections import Counter\n",
    "import chromadb\n",
    "\n",
    "def get_collection_diagnostics(show_samples: int = 0):\n",
    "    \"\"\"\n",
    "    Inspecciona la colecci√≥n persistente y devuelve:\n",
    "    - Fuentes √∫nicas\n",
    "    - Distribuci√≥n por prioridad, tipo, p√°ginas\n",
    "    - Conteo de documentos con KV\n",
    "    Opcional: muestra N ejemplos de metadatos\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=VDB_PATH)\n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "\n",
    "    data = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    metadatas = data.get(\"metadatas\", [])\n",
    "    documents = data.get(\"documents\", [])\n",
    "\n",
    "    sources = [m.get(\"source\", \"unknown\").split(\"/\")[-1] for m in metadatas if m]\n",
    "    prio_counts = Counter(m.get(\"prio\", \"general\") for m in metadatas if m)\n",
    "    type_counts = Counter(m.get(\"type\", \"text\") for m in metadatas if m)\n",
    "    page_counts = Counter(m.get(\"page\", \"na\") for m in metadatas if m)\n",
    "    kv_total = sum(1 for m in metadatas if m and \"kv\" in m)\n",
    "\n",
    "    print(f\"üì¶ Colecci√≥n: {COLLECTION_NAME}\")\n",
    "    print(f\"üìÅ Fuentes √∫nicas: {len(set(sources))} ‚Üí {sorted(set(sources))}\")\n",
    "    print(\"   üìä Distribuci√≥n por prioridad:\", dict(prio_counts))\n",
    "    print(\"   üìä Distribuci√≥n por tipo:\", dict(type_counts))\n",
    "    print(\"   üìä P√°ginas √∫nicas indexadas:\", len(page_counts))\n",
    "    print(f\"   üîë Documentos con KV: {kv_total}\")\n",
    "\n",
    "    if show_samples > 0:\n",
    "        print(\"\\nüîç Ejemplos de metadatos:\")\n",
    "        for i, m in enumerate(metadatas[:show_samples], start=1):\n",
    "            print(f\"{i}.\", m)\n",
    "\n",
    "    return {\n",
    "        \"sources\": sorted(set(sources)),\n",
    "        \"prio_counts\": dict(prio_counts),\n",
    "        \"type_counts\": dict(type_counts),\n",
    "        \"page_counts\": dict(page_counts),\n",
    "        \"kv_total\": kv_total,\n",
    "        \"total_docs\": len(documents),\n",
    "    }\n",
    "\n",
    "# Ejemplo:\n",
    "# resumen = get_collection_diagnostics(show_samples=3)\n",
    "# print(resumen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 5.1 - Chunking con preservaci√≥n de metadatos\n",
    "# ============================================\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document  # ‚úÖ Import actualizado\n",
    "\n",
    "def chunk_documents(\n",
    "    docs: List[Document],\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 100,\n",
    "    verbose: bool = False\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Divide documentos largos en chunks manejables para embeddings,\n",
    "    conservando metadatos (source, page, type, prio, kv).\n",
    "    \n",
    "    - chunk_size: tama√±o m√°ximo de cada fragmento\n",
    "    - chunk_overlap: solapamiento entre chunks\n",
    "    - verbose: si True, imprime resumen por documento\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks: List[Document] = []\n",
    "\n",
    "    for doc in docs:\n",
    "        splits = splitter.split_text(doc.page_content)\n",
    "        valid_splits = [s for s in splits if s.strip()]  # evitar basura vac√≠a\n",
    "\n",
    "        for idx, chunk in enumerate(valid_splits, start=1):\n",
    "            new_meta = doc.metadata.copy()\n",
    "            new_meta[\"chunk_id\"] = f\"{new_meta.get('page','na')}_{idx}\"\n",
    "\n",
    "            # Mantener prio y kv si existen\n",
    "            if \"prio\" not in new_meta:\n",
    "                new_meta[\"prio\"] = \"general\"\n",
    "            if \"kv\" in doc.metadata:\n",
    "                new_meta[\"kv\"] = doc.metadata[\"kv\"]\n",
    "\n",
    "            chunks.append(Document(page_content=chunk, metadata=new_meta))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"üìÑ p.{doc.metadata.get('page','?')} \"\n",
    "                  f\"| {doc.metadata.get('type','text')} \"\n",
    "                  f\"‚Üí {len(valid_splits)} chunks\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n‚úÖ Total de chunks generados: {len(chunks)}\")\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c287d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 6: correr el pipeline ---\n",
    "if __name__ == \"__main__\":\n",
    "    #reset_collection(full_reset=True)   # reinicia todo\n",
    "\n",
    "    vectorstore = add_files_to_vectordb(\n",
    "        \"./data/1.2. Anexo 1.2. H.D. ECP-PIO-23327-GCH-ST01-0-MER-HD-001- Pozos Inyectores..pdf\",\n",
    "        reset=True  # pon True si quieres borrar la colecci√≥n antes\n",
    "    )\n",
    "\n",
    "    print(\"\\nüöÄ Pipeline completado. La colecci√≥n est√° lista para consultas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccfc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # üîÑ Limpia todo si quieres partir desde cero:\n",
    "    # reset_collection()\n",
    "\n",
    "    vectorstore = add_files_to_vectordb(\n",
    "        \"./data/ECP-PIO-23327-GCH-ST01-0-MER-MR-001.pdf\",\n",
    "        reset=False  # pon True si quieres borrar la colecci√≥n antes\n",
    "    )\n",
    "\n",
    "    print(\"\\nüöÄ Pipeline completado. La colecci√≥n est√° lista para consultas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00074cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Verificar qu√© documentos hay en vectordb\n",
    "# ============================================\n",
    "import chromadb\n",
    "\n",
    "# Conectar al cliente persistente en la ruta que est√°s usando\n",
    "PERSIST_DIR = \"/home/user/Desktop/Tesis2025/AutoSelectX/scriptsSampleRAG/vectordb\"\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "\n",
    "# Nombre de la colecci√≥n que usaste al crear los embeddings\n",
    "COLLECTION_NAME = \"autoselx_docs\"\n",
    "\n",
    "def get_unique_sources_list(client, collection_name):\n",
    "    try:\n",
    "        collection = client.get_collection(collection_name)\n",
    "        # Obtener todos los documentos y metadatos\n",
    "        data = collection.get(include=[\"metadatas\"])\n",
    "        metadatas = data.get(\"metadatas\", [])\n",
    "        \n",
    "        sources = set()\n",
    "        for metadata in metadatas:\n",
    "            if metadata and \"source\" in metadata:\n",
    "                sources.add(metadata[\"source\"])\n",
    "        \n",
    "        # Extraer solo el nombre de archivo\n",
    "        file_names = sorted(set(source.split(\"/\")[-1] for source in sources))\n",
    "        return file_names\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error al acceder a la colecci√≥n: {e}\"\n",
    "\n",
    "# Ejecutar\n",
    "docs_in_vdb = get_unique_sources_list(client, COLLECTION_NAME)\n",
    "print(\"üìÇ Documentos en vectordb:\", docs_in_vdb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26552d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 7 - Debug robusto: explorar fragmentos (MMR con fallback, mejora alcance) TEST!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# ============================================\n",
    "import json\n",
    "\n",
    "def _try_retrieve(query: str, search_kwargs: dict, use_mmr: bool = True):\n",
    "    \"\"\"Envuelve as_retriever con fallback y logs amigables.\"\"\"\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\" if use_mmr else \"similarity\",\n",
    "            search_kwargs=search_kwargs,\n",
    "        )\n",
    "        return retriever.get_relevant_documents(query)\n",
    "    except Exception as e:\n",
    "        modo = \"MMR\" if use_mmr else \"similarity\"\n",
    "        print(f\"(i) Fall√≥ {modo} con error {e.__class__.__name__}: {e}\")\n",
    "        return []\n",
    "\n",
    "def debug_busqueda_alcance(\n",
    "    q: str = \"¬øCu√°l es el alcance del suministro?\",\n",
    "    k_text: int = 5,\n",
    "    k_mixed: int = 5,\n",
    "    preferir_texto: bool = True,\n",
    "):\n",
    "    print(f\"üîé Consulta: {q}\")\n",
    "\n",
    "    # --- 0) Chequeo r√°pido de colecci√≥n\n",
    "    try:\n",
    "        coll = vectorstore._collection\n",
    "        _probe = coll.get(limit=3)\n",
    "        if not _probe.get(\"ids\"):\n",
    "            print(\"(i) Colecci√≥n vac√≠a o corrupta; considera resetear y reindexar.\")\n",
    "    except Exception as e:\n",
    "        print(f\"(i) No pude inspeccionar la colecci√≥n: {e}\")\n",
    "\n",
    "    # --- 1) Fase A: priorizar TEXTO con prio ‚àà {alcance, general, hoja_datos}\n",
    "    kwargs_a = {\n",
    "        \"k\": max(k_text, 5),\n",
    "        \"fetch_k\": 40,\n",
    "        \"lambda_mult\": 0.4,\n",
    "        \"filter\": {\"prio\": {\"$in\": [\"alcance\", \"general\", \"hoja_datos\"]}},  # üëà ampliado\n",
    "    }\n",
    "\n",
    "    # Intento MMR\n",
    "    res_text = _try_retrieve(q, kwargs_a, use_mmr=True)\n",
    "    # Fallback similarity\n",
    "    if not res_text:\n",
    "        kwargs_a_fallback = {\n",
    "            \"k\": max(k_text, 5),\n",
    "            \"filter\": {\"prio\": {\"$in\": [\"alcance\", \"general\", \"hoja_datos\"]}},\n",
    "        }\n",
    "        res_text = _try_retrieve(q, kwargs_a_fallback, use_mmr=False)\n",
    "\n",
    "    # Post-filtro por tipo en Python\n",
    "    if preferir_texto:\n",
    "        res_text = [r for r in res_text if r.metadata.get(\"type\") == \"text\"]\n",
    "\n",
    "    print(f\"\\nüìö TEXT preferido (alcance/general/hoja_datos) ‚Üí {len(res_text)}\")\n",
    "\n",
    "    # --- Mostrar √∫nicos\n",
    "    seen_hash = set()\n",
    "    def _mark_and_show(prefix: str, doc, idx: int):\n",
    "        ch = doc.metadata.get(\"content_hash\") or f\"{doc.metadata.get('page')}_{doc.metadata.get('chunk_id')}\"\n",
    "        if ch in seen_hash:\n",
    "            return False\n",
    "        seen_hash.add(ch)\n",
    "        print(f\"\\n--- {prefix} {idx} ---\")\n",
    "        print((doc.page_content[:400]).replace(\"\\n\", \" \"))\n",
    "        print(\"Metadatos:\", {k: doc.metadata.get(k) for k in [\"page\",\"type\",\"prio\",\"source\",\"chunk_id\"]})\n",
    "        if \"kv\" in doc.metadata:\n",
    "            try:\n",
    "                print(\"KV:\", json.loads(doc.metadata[\"kv\"]))\n",
    "            except Exception:\n",
    "                print(\"KV (raw):\", doc.metadata[\"kv\"])\n",
    "        return True\n",
    "\n",
    "    shown = 0\n",
    "    for i, r in enumerate(res_text, 1):\n",
    "        if _mark_and_show(\"TEXT\", r, i):\n",
    "            shown += 1\n",
    "\n",
    "    # --- 2) Fase B: mezcla (text+table) como respaldo\n",
    "    kwargs_b = {\"k\": max(k_mixed, 5)}\n",
    "    res_mixed = _try_retrieve(q, kwargs_b, use_mmr=False)  # similarity estable\n",
    "    print(f\"\\nüß© MIX (text+table) ‚Üí {len(res_mixed)}\")\n",
    "\n",
    "    added = 0\n",
    "    for r in res_mixed:\n",
    "        if _mark_and_show(\"MIX\", r, added + 1):\n",
    "            added += 1\n",
    "        if added >= k_mixed:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nüìå Total √∫nicos mostrados: {len(seen_hash)}\")\n",
    "\n",
    "\n",
    "# üöÄ Ejecutar debug\n",
    "debug_busqueda_alcance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Complemento Paso 7 - Funci√≥n auxiliar de debug\n",
    "# ==========================\n",
    "def debug_query(vectorstore, query, prio=None, k=5):\n",
    "    \"\"\"\n",
    "    Ejecuta una b√∫squeda con filtro de metadatos (cuando prio != None).\n",
    "    Incluye fallbacks para prioridades relacionadas y muestra resultados\n",
    "    con metadatos enriquecidos y KV cuando existen.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    # Prios relacionadas para fallback (m√°s tolerancia)\n",
    "    prio_fallbacks = {\n",
    "        \"alcance\":    [\"alcance\", \"general\", \"hoja_datos\"],\n",
    "        \"requisito\":  [\"requisito\", \"hoja_datos\", \"general\"],\n",
    "        \"norma\":      [\"norma\", \"general\"],\n",
    "        \"hoja_datos\": [\"hoja_datos\", \"requisito\", \"general\"],\n",
    "        \"general\":    [\"general\", \"alcance\", \"hoja_datos\"]\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüîé DEBUG QUERY: '{query}' (prio={prio})\\n\")\n",
    "\n",
    "    def run_search(allowed_prios, topk):\n",
    "        if allowed_prios is None:\n",
    "            # b√∫squeda sin filtro\n",
    "            return vectorstore.similarity_search(query, k=topk)\n",
    "        else:\n",
    "            # b√∫squeda con filtro por metadatos\n",
    "            return vectorstore.similarity_search(\n",
    "                query,\n",
    "                k=topk,\n",
    "                filter={\"prio\": {\"$in\": allowed_prios}}\n",
    "            )\n",
    "\n",
    "    # --- 1) Intento principal con filtro si prio est√° definido\n",
    "    if prio:\n",
    "        allowed = prio_fallbacks.get(prio, [prio])\n",
    "        docs = run_search(allowed_prios=allowed, topk=max(k, 10))\n",
    "\n",
    "        # --- 2) Fallback si no hay resultados\n",
    "        if not docs:\n",
    "            allowed_wide = list(set(allowed + [\"general\"]))\n",
    "            docs = run_search(allowed_prios=allowed_wide, topk=50)\n",
    "\n",
    "        if not docs:\n",
    "            print(\"‚ö†Ô∏è No se encontraron resultados con esa prioridad (ni con fallback).\")\n",
    "            return\n",
    "    else:\n",
    "        # b√∫squeda sin restricci√≥n de prioridad\n",
    "        docs = run_search(allowed_prios=None, topk=k)\n",
    "\n",
    "    # --- Mostrar resultados\n",
    "    print(f\"üìä Fragmentos recuperados: {len(docs)}\\n\")\n",
    "    seen = set()\n",
    "    for i, r in enumerate(docs, 1):\n",
    "        ch = r.metadata.get(\"content_hash\") or f\"{r.metadata.get('page')}_{r.metadata.get('chunk_id')}\"\n",
    "        if ch in seen:\n",
    "            continue\n",
    "        seen.add(ch)\n",
    "\n",
    "        print(f\"--- CHUNK {i} ---\")\n",
    "        print(r.page_content[:400].replace(\"\\n\", \" \"))\n",
    "        print(\"Metadatos:\", {k: r.metadata.get(k) for k in [\"page\",\"type\",\"prio\",\"source\",\"chunk_id\"]})\n",
    "        if \"kv\" in r.metadata:\n",
    "            try:\n",
    "                print(\"KV dict:\", json.loads(r.metadata[\"kv\"]))\n",
    "            except Exception:\n",
    "                print(\"KV (raw):\", r.metadata[\"kv\"])\n",
    "        print()\n",
    "\n",
    "    print(f\"üìå Total √∫nicos mostrados: {len(seen)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f36f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Pruebas de debug por prioridad\n",
    "# ==========================\n",
    "\n",
    "# Alcance del suministro\n",
    "debug_query(vectorstore, \"alcance del suministro\", prio=\"alcance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_query(vectorstore, \"API 675 caudal presi√≥n material\", prio=\"hoja_datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_query(vectorstore, \"RETIE y normas aplicables\", prio=\"norma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_query(vectorstore, \"requisitos t√©cnicos del sistema de dosificaci√≥n\", prio=\"requisito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c9e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 8 - Probar extracci√≥n de Hoja de Datos API 675\n",
    "# ============================================\n",
    "import json\n",
    "\n",
    "def buscar_hojas_datos(pregunta: str = \"hoja de datos bomba\", k: int = 12):\n",
    "    # --- 1) Buscamos primero chunks con prio=hoja_datos ---\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    results = retriever.get_relevant_documents(pregunta)\n",
    "    hd_chunks = [r for r in results if r.metadata.get(\"prio\") == \"hoja_datos\"]\n",
    "\n",
    "    # --- 2) Si hay pocos resultados, ampliamos con los que tengan KV ---\n",
    "    if len(hd_chunks) < 3:\n",
    "        kv_chunks = [r for r in results if \"kv\" in r.metadata]\n",
    "        filtrados = hd_chunks + kv_chunks\n",
    "    else:\n",
    "        filtrados = hd_chunks\n",
    "\n",
    "    print(f\"üîé Total de chunks candidatos (HD/kv): {len(filtrados)}\")\n",
    "\n",
    "    count = 0\n",
    "    for r in filtrados:\n",
    "        has_kv = \"kv\" in r.metadata\n",
    "        if has_kv:\n",
    "            count += 1\n",
    "\n",
    "        flag = \"üîë\" if has_kv else \"\"\n",
    "        print(f\"\\n--- CHUNK HD {flag} ---\")\n",
    "        print(f\"p.{r.metadata.get('page')} | {r.metadata.get('type')} | prio={r.metadata.get('prio')}\")\n",
    "        print(r.page_content[:400])\n",
    "\n",
    "        if has_kv:\n",
    "            try:\n",
    "                kv = json.loads(r.metadata[\"kv\"])\n",
    "                print(\"KV:\", kv)\n",
    "            except Exception:\n",
    "                print(\"KV (raw):\", r.metadata[\"kv\"])\n",
    "\n",
    "    print(f\"‚úÖ Chunks con KV reales: {count}\")\n",
    "    return filtrados\n",
    "\n",
    "\n",
    "# Ejemplo:\n",
    "buscar_hojas_datos(\"Datos de dise√±o de bomba dosificadora API 675 (caudal, presi√≥n, HP)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 8.1 - Consolidar Hoja de Datos API 675\n",
    "# ============================================\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def extraer_campos_extras(texto: str) -> Dict[str, str]:\n",
    "    \"\"\"Regex para campos no capturados a√∫n en kv_from_text_page\"\"\"\n",
    "    t = texto.upper()\n",
    "    kv = {}\n",
    "\n",
    "    # Fluido de operaci√≥n\n",
    "    m = re.search(r\"FLUIDO\\s+DE\\s+OPERACI[√ìO]N\\s+SI\\s+([A-Z√Å√â√ç√ì√ö0-9\\s]+)\", t)\n",
    "    if m:\n",
    "        kv[\"fluido\"] = m.group(1).title()\n",
    "\n",
    "    # Viscosidad cP\n",
    "    m = re.search(r\"VISCO(SIDAD)?[^0-9]*(\\d{2,5})\\s*-\\s*(\\d{2,5})\\s*CP\", t)\n",
    "    if m:\n",
    "        kv[\"viscosidad_cps\"] = f\"{m.group(2)}-{m.group(3)}\"\n",
    "\n",
    "    # Eficiencia %\n",
    "    m = re.search(r\"EFICIENCIA.*?(\\d{1,3})\\s*%|‚â•\\s*(\\d{1,3})\", t)\n",
    "    if m:\n",
    "        kv[\"eficiencia_pct\"] = m.group(1) or m.group(2)\n",
    "\n",
    "    # Clasificaci√≥n de √°rea\n",
    "    m = re.search(r\"CLASIFICACI[√ìO]N\\s+DEL\\s+√ÅREA\\s+NO\\s+([A-Z\\s]+)\", t)\n",
    "    if m:\n",
    "        kv[\"clasificacion_area\"] = m.group(1).title()\n",
    "\n",
    "    return kv\n",
    "\n",
    "def consolidar_hoja_datos(pregunta=\"hoja de datos bomba API 675\", k=12) -> Dict[str, Any]:\n",
    "    docs = buscar_hojas_datos(pregunta, k=k)\n",
    "    consolidado = {}\n",
    "\n",
    "    for d in docs:\n",
    "        # 1) KV en metadata\n",
    "        if \"kv\" in d.metadata:\n",
    "            try:\n",
    "                kv = json.loads(d.metadata[\"kv\"])\n",
    "                consolidado.update(kv)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 2) Extra campos con regex sobre el contenido\n",
    "        extras = extraer_campos_extras(d.page_content)\n",
    "        consolidado.update(extras)\n",
    "\n",
    "    print(\"\\n‚úÖ JSON consolidado:\")\n",
    "    print(json.dumps(consolidado, indent=2, ensure_ascii=False))\n",
    "    return consolidado\n",
    "\n",
    "# Ejemplo:\n",
    "consolidar_hoja_datos(\"Datos de dise√±o de bomba dosificadora API 675\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 9 - Inspeccionar metadatos del Alcance\n",
    "# ============================================\n",
    "\n",
    "def buscar_alcance(pregunta: str = \"alcance del suministro\", k: int = 8):\n",
    "    \"\"\"Busca chunks relacionados con alcance (prio=alcance) y muestra metadatos\"\"\"\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": k, \"filter\": {\"prio\": \"alcance\"}}\n",
    "    )\n",
    "    results = retriever.get_relevant_documents(pregunta)\n",
    "\n",
    "    print(f\"üîé Total de chunks recuperados (prio=alcance): {len(results)}\")\n",
    "    for r in results:\n",
    "        print(\n",
    "            f\"\\n--- CHUNK p.{r.metadata.get('page')} | \"\n",
    "            f\"{r.metadata.get('type')} | prio={r.metadata.get('prio')} ---\"\n",
    "        )\n",
    "        print(r.page_content[:400].replace(\"\\n\", \" \"))\n",
    "        print(\"Metadatos:\", r.metadata)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Ejemplo de prueba:\n",
    "buscar_alcance()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 10 - Validar TODOS los metadatos (corregido)\n",
    "# ============================================\n",
    "from collections import Counter\n",
    "\n",
    "def validar_metadatos_globales():\n",
    "    \"\"\"Valida consistencia de metadatos en la colecci√≥n completa.\"\"\"\n",
    "    collection = vectorstore._collection\n",
    "    data = collection.get(include=[\"metadatas\", \"documents\"])\n",
    "\n",
    "    total_docs = len(data[\"documents\"])\n",
    "    print(f\"üîé Total de registros en la colecci√≥n: {total_docs}\")\n",
    "\n",
    "    errores = 0\n",
    "    prio_counts = Counter()\n",
    "    type_counts = Counter()\n",
    "    kv_count = 0\n",
    "    seen_ids = set()\n",
    "    duplicados = 0\n",
    "\n",
    "    for i, meta in enumerate(data[\"metadatas\"]):\n",
    "        doc_id = data[\"ids\"][i]  # aqu√≠ s√≠ podemos usarlo, ya viene siempre\n",
    "\n",
    "        # Detectar duplicados de ID\n",
    "        if doc_id in seen_ids:\n",
    "            duplicados += 1\n",
    "        else:\n",
    "            seen_ids.add(doc_id)\n",
    "\n",
    "        # Validar claves m√≠nimas\n",
    "        if not all(key in meta for key in [\"source\", \"page\", \"type\"]):\n",
    "            print(f\"‚ö†Ô∏è Registro {i} con metadatos incompletos:\", meta)\n",
    "            errores += 1\n",
    "            continue\n",
    "\n",
    "        # Validar valores\n",
    "        if not isinstance(meta.get(\"page\"), int) or meta[\"page\"] <= 0:\n",
    "            print(f\"‚ö†Ô∏è Registro {i} con p√°gina inv√°lida:\", meta)\n",
    "            errores += 1\n",
    "\n",
    "        if not meta.get(\"type\"):\n",
    "            print(f\"‚ö†Ô∏è Registro {i} con tipo vac√≠o:\", meta)\n",
    "            errores += 1\n",
    "\n",
    "        # Contadores √∫tiles\n",
    "        prio_counts[meta.get(\"prio\", \"none\")] += 1\n",
    "        type_counts[meta.get(\"type\", \"unknown\")] += 1\n",
    "        if \"kv\" in meta:\n",
    "            kv_count += 1\n",
    "\n",
    "    # Resumen\n",
    "    if errores == 0:\n",
    "        print(\"‚úÖ Todos los documentos tienen metadatos completos y v√°lidos.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Se encontraron {errores} problemas de metadatos.\")\n",
    "\n",
    "    if duplicados > 0:\n",
    "        print(f\"‚ö†Ô∏è Se detectaron {duplicados} duplicados de IDs.\")\n",
    "\n",
    "    print(\"\\nüìä Distribuci√≥n por prioridad:\", dict(prio_counts))\n",
    "    print(\"üìä Distribuci√≥n por tipo:\", dict(type_counts))\n",
    "    print(f\"üìä Total de documentos con KV (tablas clave-valor): {kv_count}\")\n",
    "\n",
    "# Ejemplo:\n",
    "validar_metadatos_globales()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 11 - Buscar solo tablas o solo texto\n",
    "# ============================================\n",
    "def buscar_por_tipo(tipo: str = \"table\", pregunta: str = \"bomba dosificadora\", k: int = 5):\n",
    "    \"\"\"\n",
    "    Recupera fragmentos filtrando por tipo ('table' o 'text').\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": k, \"filter\": {\"type\": tipo}}\n",
    "    )\n",
    "    results = retriever.get_relevant_documents(pregunta)\n",
    "\n",
    "    print(f\"\\nüîé Resultados filtrados por tipo='{tipo}' | pregunta='{pregunta}' ‚Üí {len(results)} encontrados\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n--- CHUNK {i} ---\")\n",
    "        print(r.page_content[:400].replace(\"\\n\", \" \"))\n",
    "        print(\"Metadatos:\", r.metadata)\n",
    "\n",
    "        if \"kv\" in r.metadata:  # Mostrar KV si existe\n",
    "            try:\n",
    "                import json\n",
    "                print(\"KV:\", json.loads(r.metadata[\"kv\"]))\n",
    "            except Exception:\n",
    "                print(\"KV (raw):\", r.metadata[\"kv\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Ejemplos de prueba:\n",
    "buscar_por_tipo(\"table\", \"tags y cantidades\")\n",
    "buscar_por_tipo(\"text\", \"alcance del suministro\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 11.1 - Comparar resultados table vs text\n",
    "# ============================================\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "def comparar_table_vs_text(pregunta=\"alcance del suministro\", k=6):\n",
    "    resultados = {}\n",
    "    for tipo in [\"table\", \"text\"]:\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": k, \"filter\": {\"type\": tipo}}\n",
    "        )\n",
    "        docs = retriever.get_relevant_documents(pregunta)\n",
    "        resultados[tipo] = docs\n",
    "\n",
    "        print(f\"\\nüîé Resultados filtrados por tipo='{tipo}' ‚Üí {len(docs)} encontrados\")\n",
    "        prio_counter = Counter([d.metadata.get(\"prio\", \"none\") for d in docs])\n",
    "        kv_count = sum(1 for d in docs if \"kv\" in d.metadata)\n",
    "\n",
    "        print(f\"üìä Distribuci√≥n de prioridades: {dict(prio_counter)}\")\n",
    "        print(f\"üìä Con KV extra√≠dos: {kv_count}\")\n",
    "\n",
    "        for r in docs:\n",
    "            print(\"\\n--- CHUNK ---\")\n",
    "            print(r.page_content[:300].replace(\"\\n\", \" \"))\n",
    "            print(\"Metadatos:\", r.metadata)\n",
    "            if \"kv\" in r.metadata:\n",
    "                try:\n",
    "                    print(\"KV:\", json.loads(r.metadata[\"kv\"]))\n",
    "                except:\n",
    "                    print(\"KV (raw):\", r.metadata[\"kv\"])\n",
    "\n",
    "    # Comparaci√≥n final\n",
    "    print(\"\\n================= RESUMEN COMPARATIVO =================\")\n",
    "    for tipo in [\"table\", \"text\"]:\n",
    "        print(f\"Tipo: {tipo} | Total: {len(resultados[tipo])}\")\n",
    "        kv_total = sum(1 for d in resultados[tipo] if \"kv\" in d.metadata)\n",
    "        print(f\" ‚Üí Con KV: {kv_total}\")\n",
    "        prio_counter = Counter([d.metadata.get(\"prio\", \"none\") for d in resultados[tipo]])\n",
    "        print(f\" ‚Üí Prioridades: {dict(prio_counter)}\")\n",
    "\n",
    "# Ejemplo de prueba\n",
    "comparar_table_vs_text(\"tags y cantidades\", k=5)\n",
    "comparar_table_vs_text(\"alcance del suministro\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f39337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 12 - Buscar en una p√°gina espec√≠fica\n",
    "# ============================================\n",
    "def buscar_por_pagina(pagina: int, pregunta: str):\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 5, \"filter\": {\"page\": pagina}}\n",
    "    )\n",
    "    results = retriever.get_relevant_documents(pregunta)\n",
    "\n",
    "    print(f\"üîé Resultados en p√°gina {pagina}: {len(results)}\")\n",
    "    for r in results:\n",
    "        print(\"\\n--- CHUNK ---\")\n",
    "        print(r.page_content[:400].replace(\"\\n\", \" \"))\n",
    "        print(\"Metadatos:\", r.metadata)\n",
    "\n",
    "# Ejemplo:\n",
    "buscar_por_pagina(32, \"alcance del suministro\")\n",
    "buscar_por_pagina(42, \"caudal de la bomba\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d354f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 13.1 ‚Äì Inventario de sistemas (Skids, TAGs y cantidades)\n",
    "# ============================================\n",
    "\n",
    "import json\n",
    "\n",
    "def prueba_inventario_sistemas():\n",
    "    pregunta = \"¬øCu√°les son los skids o sistemas de dosificaci√≥n requeridos? Dame los TAG y las cantidades.\"\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 10, \"filter\": {\"type\": \"table\"}}\n",
    "    )\n",
    "    results = retriever.get_relevant_documents(pregunta)\n",
    "\n",
    "    print(f\"üîé Resultados prueba INVENTARIO (skids + tags + cantidades): {len(results)}\")\n",
    "\n",
    "    kv_count = 0\n",
    "    for r in results:\n",
    "        print(\"\\n--- CHUNK INVENTARIO ---\")\n",
    "        print(r.page_content[:400].replace(\"\\n\", \" \"))\n",
    "        print(\"Metadatos:\", r.metadata)\n",
    "\n",
    "        if \"kv\" in r.metadata:\n",
    "            try:\n",
    "                kv_dict = json.loads(r.metadata[\"kv\"])\n",
    "                print(\"KV dict:\", kv_dict)\n",
    "                kv_count += 1\n",
    "            except Exception:\n",
    "                print(\"KV (raw):\", r.metadata[\"kv\"])\n",
    "\n",
    "    print(f\"\\nüìä Total de chunks con KV √∫tiles: {kv_count}\")\n",
    "\n",
    "# Ejemplo de ejecuci√≥n\n",
    "prueba_inventario_sistemas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Paso 14 (v3) ‚Äì Alcance + Datos de dise√±o de la bomba API 675 (robusto + actualizado)\n",
    "# ============================================\n",
    "import re, json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def _j(x):\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _parse_nums(text: str):\n",
    "    \"\"\"Extrae caudal (GPH), presi√≥n (psig) y HP desde texto libre (fallback).\"\"\"\n",
    "    caudal = None\n",
    "    pres = None\n",
    "    hp = None\n",
    "\n",
    "    # Caudal \"12 - 18 GPH\" | \"12‚Äì18 Gph\" | \"12 a 18 GPH\" | simple \"18 GPH\"\n",
    "    m = re.search(r'(\\d+(?:[.,]\\d+)?)\\s*(?:-|‚Äì|a|to)\\s*(\\d+(?:[.,]\\d+)?)\\s*(gph|g\\.?p\\.?h\\.?)\\b', text, re.I)\n",
    "    if m:\n",
    "        caudal = f\"{m.group(1)} - {m.group(2)}\"\n",
    "    else:\n",
    "        m = re.search(r'\\b(\\d+(?:[.,]\\d+)?)\\s*(gph|g\\.?p\\.?h\\.?)\\b', text, re.I)\n",
    "        if m:\n",
    "            caudal = m.group(1)\n",
    "\n",
    "    # Presi√≥n \"@ 100 psi\" | \"100 psig\"\n",
    "    m = re.search(r'@?\\s*(\\d+(?:[.,]\\d+)?)\\s*(psi|psig)\\b', text, re.I)\n",
    "    if m:\n",
    "        pres = m.group(1)\n",
    "\n",
    "    # Potencia \"1 HP\"\n",
    "    m = re.search(r'\\b(\\d+(?:[.,]\\d+)?)\\s*hp\\b', text, re.I)\n",
    "    if m:\n",
    "        hp = m.group(1)\n",
    "\n",
    "    return caudal, pres, hp\n",
    "\n",
    "def prueba_alcance_y_datos_bomba_v3():\n",
    "    q_alc = \"alcance del suministro paquete de inyecci√≥n de qu√≠micos\"\n",
    "    q_hd  = \"bomba dosificadora API 675 caudal presi√≥n HP materiales GPH psig\"\n",
    "\n",
    "    # 1) Recuperar alcance\n",
    "    retr_alc = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 12, \"filter\": {\"prio\": \"alcance\"}},\n",
    "        search_type=\"mmr\"\n",
    "    )\n",
    "    alc_docs = retr_alc.get_relevant_documents(q_alc)\n",
    "\n",
    "    if not alc_docs:  # fallback\n",
    "        retr_alc_fb = vectorstore.as_retriever(search_kwargs={\"k\": 12}, search_type=\"mmr\")\n",
    "        alc_docs = retr_alc_fb.get_relevant_documents(\"alcance del suministro\")\n",
    "\n",
    "    # 2) Recuperar hojas de datos\n",
    "    retr_hd = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 20, \"filter\": {\"prio\": \"hoja_datos\"}},\n",
    "        search_type=\"mmr\"\n",
    "    )\n",
    "    hd_docs = retr_hd.get_relevant_documents(q_hd)\n",
    "\n",
    "    # 3) Refuerzo directo desde la colecci√≥n (KV + texto clave)\n",
    "    coll = vectorstore._collection\n",
    "    meta_hd = coll.get(where={\"prio\": \"hoja_datos\"}, include=[\"metadatas\", \"documents\"])\n",
    "    for md, doc in zip(meta_hd[\"metadatas\"], meta_hd[\"documents\"]):\n",
    "        if not isinstance(md, dict):\n",
    "            continue\n",
    "        if (\"kv\" in md and any(k in md[\"kv\"] for k in [\"caudal_gph\", \"presion_psig\", \"hp_motor\", \"material_tanque\", \"material_mojado\", \"bombas_operativas\", \"bombas_respaldo\"])) \\\n",
    "           or (\"API 675\" in (doc or \"\") or \"GPH\" in (doc or \"\") or \"psig\" in (doc or \"\")):\n",
    "            hd_docs.append(Document(page_content=doc, metadata=md))\n",
    "\n",
    "    # 4) De-duplicar por (page,type,primeros 60 chars)\n",
    "    def _key(d): return (d.metadata.get(\"page\"), d.metadata.get(\"type\"), (d.page_content or \"\")[:60])\n",
    "    seen = set()\n",
    "    alc_unique, hd_unique = [], []\n",
    "    for d in alc_docs:\n",
    "        k = _key(d)\n",
    "        if k not in seen:\n",
    "            seen.add(k); alc_unique.append(d)\n",
    "    for d in hd_docs:\n",
    "        k = _key(d)\n",
    "        if k not in seen:\n",
    "            seen.add(k); hd_unique.append(d)\n",
    "\n",
    "    # 5) Extraer KV + regex ‚Üí s√≠ntesis\n",
    "    datos = {\n",
    "        \"caudal_gph\": None,\n",
    "        \"presion_psig\": None,\n",
    "        \"hp_motor\": None,\n",
    "        \"materiales\": set(),\n",
    "        \"api_675\": False,\n",
    "        \"bombas_operativas\": None,\n",
    "        \"bombas_respaldo\": None,\n",
    "        \"paginas_fuente\": set(),\n",
    "    }\n",
    "\n",
    "    for d in hd_unique:\n",
    "        md = d.metadata or {}\n",
    "        if \"page\" in md:\n",
    "            datos[\"paginas_fuente\"].add(md[\"page\"])\n",
    "\n",
    "        # KV directo\n",
    "        if \"kv\" in md:\n",
    "            kv = _j(md[\"kv\"])\n",
    "            if not datos[\"caudal_gph\"] and kv.get(\"caudal_gph\"):\n",
    "                datos[\"caudal_gph\"] = kv[\"caudal_gph\"]\n",
    "            if not datos[\"presion_psig\"] and kv.get(\"presion_psig\"):\n",
    "                datos[\"presion_psig\"] = kv[\"presion_psig\"]\n",
    "            if not datos[\"hp_motor\"] and kv.get(\"hp_motor\"):\n",
    "                datos[\"hp_motor\"] = kv[\"hp_motor\"]\n",
    "            if kv.get(\"material_tanque\"):\n",
    "                datos[\"materiales\"].add(kv[\"material_tanque\"])\n",
    "            if kv.get(\"material_mojado\"):\n",
    "                datos[\"materiales\"].add(kv[\"material_mojado\"])\n",
    "            if str(kv.get(\"api_675\")).lower() == \"true\":\n",
    "                datos[\"api_675\"] = True\n",
    "            if kv.get(\"bombas_operativas\"):\n",
    "                datos[\"bombas_operativas\"] = kv[\"bombas_operativas\"]\n",
    "            if kv.get(\"bombas_respaldo\"):\n",
    "                datos[\"bombas_respaldo\"] = kv[\"bombas_respaldo\"]\n",
    "\n",
    "        # Fallback regex\n",
    "        if any(v is None for v in (datos[\"caudal_gph\"], datos[\"presion_psig\"], datos[\"hp_motor\"])):\n",
    "            c, p, h = _parse_nums(d.page_content or \"\")\n",
    "            if c and not datos[\"caudal_gph\"]:\n",
    "                datos[\"caudal_gph\"] = c\n",
    "            if p and not datos[\"presion_psig\"]:\n",
    "                datos[\"presion_psig\"] = p\n",
    "            if h and not datos[\"hp_motor\"]:\n",
    "                datos[\"hp_motor\"] = h\n",
    "\n",
    "    # 6) Mostrar resultados\n",
    "    print(f\"üîé ALCANCE (k={len(alc_unique)}):\")\n",
    "    for r in alc_unique[:5]:\n",
    "        print(f\"\\n‚Äî p.{r.metadata.get('page')} | {r.metadata.get('type')}\")\n",
    "        print((r.page_content or \"\")[:350].replace(\"\\n\", \" \"))\n",
    "\n",
    "    print(f\"\\nüîé HOJA DE DATOS (k={len(hd_unique)}):\")\n",
    "    for r in hd_unique[:6]:\n",
    "        print(f\"\\n‚Äî p.{r.metadata.get('page')} | {r.metadata.get('type')}\")\n",
    "        print((r.page_content or \"\")[:350].replace(\"\\n\", \" \"))\n",
    "        if \"kv\" in r.metadata:\n",
    "            print(\"KV:\", r.metadata[\"kv\"])\n",
    "\n",
    "    print(\"\\n‚úÖ EXTRACCI√ìN SINTETIZADA:\")\n",
    "    print(f\"  ‚Ä¢ Caudal (GPH): {datos['caudal_gph']}\")\n",
    "    print(f\"  ‚Ä¢ Presi√≥n (psig): {datos['presion_psig']}\")\n",
    "    print(f\"  ‚Ä¢ Potencia (HP): {datos['hp_motor']}\")\n",
    "    print(f\"  ‚Ä¢ Materiales: {', '.join(sorted(datos['materiales'])) or '‚Äî'}\")\n",
    "    print(f\"  ‚Ä¢ API 675: {'S√≠' if datos['api_675'] else 'N/D'}\")\n",
    "    print(f\"  ‚Ä¢ Bombas operativas: {datos['bombas_operativas'] or '‚Äî'}\")\n",
    "    print(f\"  ‚Ä¢ Bombas respaldo: {datos['bombas_respaldo'] or '‚Äî'}\")\n",
    "    print(f\"  ‚Ä¢ P√°ginas fuente (HD): {sorted(datos['paginas_fuente'])}\")\n",
    "\n",
    "    return {\"alcance\": alc_unique, \"hoja_datos\": hd_unique, \"datos\": datos}\n",
    "\n",
    "# Ejecuci√≥n:\n",
    "prueba_alcance_y_datos_bomba_v3()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
