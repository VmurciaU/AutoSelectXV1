
#Se eejecutan asi los archivos

python RAGGrafo/scripts/pc1_read_pdfs.py
python RAGGrafo/scripts/pc2_clean_layout.py
python RAGGrafo/scripts/pc3_parse_blocks.py
python RAGGrafo/scripts/pc4_consolidate.py
python RAGGrafo/scripts/pc5_graph_build.py
python RAGGrafo/scripts/pc6_lightrag.py --use-core all
python RAGGrafo/scripts/upload_graph.py

python RAGGrafo/scripts/benchmark_queries.py # Test de varias preguntas



#Se levanta LighrRAG con ollama **** no probado aun

# modelos recomendados en CPU
export LIGHTRAG_EMBEDDING_MODEL="BAAI/bge-m3"
export LIGHTRAG_RERANK_MODEL="BAAI/bge-reranker-v2-m3"

# arranca el server (puerto 8777)
lightrag-server --host 0.0.0.0 --port 8777








# DetÃ©n server anterior (Ctrl+C)
# Exporta estas dos *ademÃ¡s* de las que ya tienes
export LLM_MODEL="gpt-4o-mini"
export EMBEDDING_MODEL="text-embedding-3-large"

# Arranca
lightrag-server \
  --host 0.0.0.0 \
  --port 8777 \
  --working-dir ./rag_storage \
  --llm-binding openai \
  --embedding-binding openai \
  --rerank-binding null


#Para probar los parametros
curl -s http://localhost:8777/health | jq '.configuration | {llm_binding,llm_model,embedding_binding,embedding_model,summary_language}'



# Exportacion previo arranque*************************
# Clave (cÃ¡mbiala/rota tu clave: la publicaste en consola)
export OPENAI_API_KEY="sk-...."

# Binding
export LIGHTRAG_LLM_BINDING=openai
export LIGHTRAG_EMBEDDING_BINDING=openai

# NOMBRES QUE SÃ LEE primero:
export LLM_MODEL=gpt-4o-mini
export EMBEDDING_MODEL=text-embedding-3-large
export EMBEDDING_DIM=3072

# (Opcionales; Ãºtiles, pero no garantizados segÃºn versiÃ³n)
export LIGHTRAG_LLM_MODEL=gpt-4o-mini
export LIGHTRAG_EMBEDDING_MODEL=text-embedding-3-large
export LIGHTRAG_EMBEDDING_DIM=3072
export NANO_VECTORDB_DIM=3072

# Idioma
export LIGHTRAG_SUMMARY_LANGUAGE=Spanish


#Arrancar servidor
lightrag-server \
  --host 0.0.0.0 \
  --port 8777 \
  --working-dir ./rag_storage \
  --llm-binding openai \
  --embedding-binding openai \
  --rerank-binding null



#En otro terminar se consoltan los parametros
curl -s http://localhost:8777/health | jq '.configuration | {llm_model, embedding_model}'
# => gpt-4o-mini / text-embedding-3-large

#En otro terminal. Subida y prueb de recuperacion
curl -s -X POST http://localhost:8777/documents/upload -F "file=@/ruta/a/archivo.ext" | jq
curl -s -X POST http://localhost:8777/query/data \
  -H "Content-Type: application/json" \
  -d '{"query":"tu consulta","mode":"naive","top_k":10}' | jq




************* IMPORTANTE
esto lo haremos luego
â†’ âš ï¸ â€œNo tengo suficiente informaciÃ³nâ€¦â€
Eso es normal: los bloques PC-6 del corpus no incluyen tablas FAT (sÃ³lo HD, MR, ET).
ğŸ‘‰ Cuando generes el nuevo corpus PC-7, asegÃºrate de incluir las tablas tipo â€œMaterial List / BOMâ€ exportadas por pdfplumber en CSV â†’ JSON para que LightRAG las indexe.





        args.queries = [
            "Â¿La especificacion solicita bombas dosificadoras? Dame los Tags, las cantidades y cual parte del documento son solicitadas",
            "Â¿CuÃ¡l es el caudal nominal, presiÃ³n de trabajo, viscosidad y el turndown de la o las bombas dosificadoras?",
            "Â¿QuÃ© normas aplican a FAT y cÃ³mo se vinculan con el paquete de inyecciÃ³n?",
            "Conecta Bomba Dosificadora â†’ Prueba FAT â†’ API 675; indica el camino y la justificaciÃ³n.",
            "Dame setpoint y la instrumentacion y tipo de instrumentos requeridos para el equipo de dosificaciÃ³n (si aplica).",
        ]



# A) Pregunta directa (modo mix recomendado)
curl -s -X POST http://localhost:8777/query \
  -H "Content-Type: application/json" \
  -d '{"query":"Â¿CuÃ¡l es el caudal nominal y el turndown de la bomba dosificadora?","mode":"mix"}' | jq

python RAGGrafo/scripts/benchmark_queries_v2.py --modes naive local global mix --style concise --wrap 110
